{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7df94e3",
   "metadata": {},
   "source": [
    "# Coll Docu Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56f5d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Libraries required\n",
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install pillow\n",
    "# !pip install img2pdf\n",
    "# !pip install xlsxwriter\n",
    "# !pip install html5lib\n",
    "# !pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af0eb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import urllib.request\n",
    "import glob\n",
    "from PIL import Image, ImageSequence\n",
    "import img2pdf\n",
    "import numpy as np\n",
    "import shutil\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1ed097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "PATH = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "DOWNLOAD_PATH = 'C:/Users/ebryaga/Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5853d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a564a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(PATH)\n",
    "\n",
    "driver.get('http://edmdote.bakerhughes.com:8080/ged-html/input.thtml?tipol=ALL')\n",
    "\n",
    "# search = driver.find_element_by_name(\"matr_macchina\")\n",
    "# search.send_keys(\"G06500\")\n",
    "\n",
    "# print(driver.title)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84cccb1",
   "metadata": {},
   "source": [
    "## Sign in in Pop up window to Coll DOcu using SSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00eda037",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('http://edmdote.bakerhughes.com:8080/ged-html/input.thtml?tipol=ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b407c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = driver.find_element('xpath', '/html/body/form/p[1]/table/tbody/tr[3]/td[2]/select').text.split('\\n')\n",
    "table_names_to_full_name = {x.split('-')[1]:x for x in table_names}\n",
    "table_names_to_num = {x.split('-')[1]:x.split('-')[0] for x in table_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb0d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(x):\n",
    "    '''\n",
    "    This function enables you to serach documents by Serial Numbers.\n",
    "    \n",
    "    Args:\n",
    "        x - serial number\n",
    "        \n",
    "    Returns:\n",
    "        Widnow is switched for searched SN documentation.\n",
    "    '''\n",
    "    \n",
    "    search_window = driver.window_handles[0]   #  Defining window/page\n",
    "    driver.switch_to.window(search_window)     # Switching between widnows/pages\n",
    "#     search = driver.find_element_by_name(\"matr_macchina\")    # Old version\n",
    "    search = driver.find_element(\"name\", \"matr_macchina\")\n",
    "    search.clear()     # Clear selected textbox\n",
    "    search.send_keys(x) # Fill selected textbox with x - function input\n",
    "    \n",
    "    database = driver.find_element('xpath', '/html/body/form/p[1]/table/tbody/tr[1]/td[2]/select')\n",
    "    database.send_keys(\"ALL\")\n",
    "    \n",
    "    search_limit = driver.find_element('xpath', '/html/body/form/p[3]/input')\n",
    "    search_limit.clear()\n",
    "    search_limit.send_keys(\"10000\")\n",
    "    \n",
    "#     submit = driver.find_element('xpath', '/html/body/form/center[1]/p/a[1]/img')\n",
    "    submit = driver.find_element('xpath', '/html/body/form/center[2]/a[1]/button')\n",
    "    submit.click()\n",
    "    \n",
    "    #switch to results\n",
    "    new_window = driver.window_handles[1]\n",
    "    driver.switch_to.window(new_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33787148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(x):\n",
    "    '''\n",
    "    This function creates Excel spreadsheet with overview of found documents in Coll Docu for given SN and also\n",
    "    saving those tables in json format.\n",
    "    This function is not executing searching process again. The input SNs is needed only for file naming purposes.\n",
    "    \n",
    "    Args:\n",
    "        x - serial number\n",
    "        \n",
    "    Returns:\n",
    "        {SN}.json - saving dict with tables\n",
    "        SN_{SN_num}.xlsx  - spreadsheet with tables (same as json, but saved in different format)\n",
    "    '''\n",
    "    search_window = driver.window_handles[0]\n",
    "    new_window = driver.window_handles[1]\n",
    "    driver.switch_to.window(new_window)\n",
    "    source = driver.page_source\n",
    "    \n",
    "    # Get tables for result page\n",
    "    try:\n",
    "        df = pd.read_html(source.replace('100%','1'))\n",
    "        my_dict = {}\n",
    "        for d in df:\n",
    "            if type(d.iloc[0,0]) != np.float64:\n",
    "                my_key = d.iloc[0,0].replace('\\xa0',' ').split(' ')[1]\n",
    "                if my_key in my_dict.keys():\n",
    "                    df_1 = my_dict[my_key]\n",
    "                    my_value = d.iloc[1:,:]\n",
    "                    my_value.columns = my_value.iloc[0]\n",
    "                    my_value = my_value.iloc[1:,:]\n",
    "                    df_full = df_1.append(my_value)\n",
    "                    my_dict[my_key] = df_full\n",
    "                else:\n",
    "                    my_value = d.iloc[1:,:]\n",
    "                    my_value.columns = my_value.iloc[0]\n",
    "                    my_value = my_value.iloc[1:,:]\n",
    "                    my_dict[my_key] = my_value\n",
    "                \n",
    "        # Saving to json\n",
    "        with open(f'{x}.json', 'wb') as fp:\n",
    "            pickle.dump(my_dict, fp)\n",
    "\n",
    "        # Saving to excel   \n",
    "        writer = pd.ExcelWriter(f'SN_{x}.xlsx', engine='xlsxwriter')\n",
    "        for key, value in my_dict.items():\n",
    "            value.to_excel(writer, sheet_name=key)\n",
    "        writer.save()\n",
    "    \n",
    "    except ValueError:\n",
    "        print('Whoops')\n",
    "        \n",
    "    driver.switch_to.window(search_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dc0e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_links(x):\n",
    "    '''\n",
    "    This function creates a list of links (xpaths) for attachments.\n",
    "    \n",
    "    Args:\n",
    "        x - serial number (only for namin purposes?)\n",
    "        \n",
    "    Returns:\n",
    "        links - list, containing links (xpaths) for ALL attachments\n",
    "    \n",
    "    '''\n",
    "    search_window = driver.window_handles[0]\n",
    "    new_window = driver.window_handles[1]\n",
    "    driver.switch_to.window(new_window)\n",
    "    source = driver.page_source\n",
    "    try:\n",
    "        df = pd.read_html(source.replace('100%','1'))\n",
    "        my_attachments = {}        #{'Table 17': [indexy, bez duplikatow]}\n",
    "        table = 1\n",
    "        for d in df:\n",
    "            if type(d.iloc[0,0]) != np.float64:\n",
    "                my_key = d.iloc[0,0].replace('\\xa0',' ').split(' ')[1]\n",
    "                my_value = d.iloc[1:,:]\n",
    "                my_value.columns = my_value.iloc[0]\n",
    "                my_value = my_value.iloc[1:,:]\n",
    "            \n",
    "            \n",
    "            \n",
    "            if int(my_key) == 17:\n",
    "                a = my_value[['N.FRAMES', 'JOB.', 'JOB.REV.']]\n",
    "                att_indexes = [2] + a[a.shift(-1) != a].dropna(how='all').index.to_list()\n",
    "                my_attachments[table] = att_indexes\n",
    "    #             my_attachments[my_key] = my_value.drop_duplicates(subset=['N.FRAMES', 'JOB.', 'JOB.REV.']).index.to_list()\n",
    "            else:\n",
    "                a = my_value[['N.FRAMES', 'JOB.']]\n",
    "                att_indexes = [2] + a[a.shift(-1) != a].dropna(how='all').index.to_list()\n",
    "                att_indexes = list(set(att_indexes))\n",
    "                my_attachments[table] = att_indexes\n",
    "\n",
    "            table += 1\n",
    "    \n",
    "        links = []\n",
    "        for key, value in my_attachments.items():\n",
    "            for y in value:\n",
    "                updated_value = int(y) + 1\n",
    "                link = f'/html/body/form/b[2]/table[{key}]/tbody/tr[{updated_value}]/td[1]/a'\n",
    "                links.append(link)\n",
    "                \n",
    "        return links\n",
    "    \n",
    "    except ValueError:\n",
    "        print('Whoops')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7a29ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full_version\n",
    "\n",
    "def download_filter_attachements(x, links):\n",
    "    '''\n",
    "    This function downloads files based on list of links (xpaths) and moves it to dedicated folder for each SN.\n",
    "    \n",
    "    Args:\n",
    "        x - serial number (only for namin purposes?)\n",
    "        links - list of xpaths, this is a result from \"get_att_links\" function\n",
    "        \n",
    "    Returns:\n",
    "        download file - depneds of type of donloaded file:\n",
    "                        pdf - normal download\n",
    "                        tiff - download and covert to pdf format\n",
    "    \n",
    "    '''   \n",
    "    i = 1\n",
    "    tables = []\n",
    "    jobs = []\n",
    "    pages = []\n",
    "    org_directory = os.getcwd()\n",
    "#     attachments = driver.find_elements_by_xpath('/html/body/form/b[2]//a')[:-1]\n",
    "    for att_path in links:\n",
    "        print(att_path)\n",
    "        attachment = driver.find_element('xpath', att_path)\n",
    "        attachment.click()\n",
    "        att_window = driver.window_handles[-1]\n",
    "        driver.switch_to.window(att_window)\n",
    "        try:\n",
    "            download = driver.find_element('xpath', '/html/body/form/center[2]/a')\n",
    "        except:\n",
    "            download = driver.find_element('xpath', '/html/body/form/a')\n",
    "        table = driver.find_element('xpath', '/html/body/form/table/tbody/tr[1]/td[2]').text\n",
    "        job = driver.find_element('xpath', '/html/body/form/table/tbody/tr[5]/td[2]').text\n",
    "        page = driver.find_elements('xpath', '/html/body/form/table/tbody/tr/td[2]')[-1].text\n",
    "            \n",
    "\n",
    "        table = table_names_to_num[table]\n",
    "        if int(table) == 17:\n",
    "            reapir_job = driver.find_element('xpath', '/html/body/form/table/tbody/tr[6]/td[2]').text\n",
    "       \n",
    "        if len(tables) == 0:\n",
    "            path = f'{org_directory}\\\\{table}'\n",
    "            os.mkdir(path)\n",
    "            os.chdir(path)\n",
    "        if (len(tables) > 0) and table != tables[-1]:\n",
    "            path = f'{org_directory}\\\\{table}'\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "                i = 1\n",
    "            os.chdir(path)       \n",
    "        if page == 'COLL/SERM':\n",
    "            print(\"Page not found\")\n",
    "            page = \"0\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (len(tables) == 0) or (len(tables) >= 1 and (table, job, page) != (tables[-1], jobs[-1], pages[-1])):\n",
    "            tables.append(table)\n",
    "            jobs.append(job)\n",
    "            pages.append(page)\n",
    "            try:\n",
    "                download = driver.find_element('xpath','/html/body/form/center[2]/a')\n",
    "            except:\n",
    "                download = driver.find_element('xpath','/html/body/form/a')\n",
    "            current_url = driver.current_url\n",
    "            download.click()\n",
    "            \n",
    "            if int(table) == 17:\n",
    "#                 reapir_job = driver.find_element_by_xpath('/html/body/form/table/tbody/tr[6]/td[2]').text\n",
    "                if driver.current_url != current_url:\n",
    "                    try:\n",
    "                        download_file(driver.current_url, f'SN_{x}_{table}_{job}_RJ{reapir_job}_{page}p_{i}')\n",
    "                    except:\n",
    "                        print('Download went wrong', x, table, job)\n",
    "#                     download_file(driver.current_url, f'SN_{x}_{table}_{job}_RJ{reapir_job}_{page}p_{i}')\n",
    "                    i += 1   \n",
    "                else:\n",
    "                    downloaded = False\n",
    "                    while not downloaded:\n",
    "                        try:\n",
    "                            list_of_files = glob.glob(f'{DOWNLOAD_PATH}/*')\n",
    "                            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "                            norm_latest_file = str(os.path.normpath(latest_file))\n",
    "                            downloaded = True\n",
    "                        except FileNotFoundError:\n",
    "                            time.sleep(2)\n",
    "                                                                    \n",
    "\n",
    "                    while (norm_latest_file.split('.')[-1] != 'tiff'):\n",
    "                        time.sleep(2)\n",
    "                        list_of_files = glob.glob(f'{DOWNLOAD_PATH}/*') # * means all if need specific format then *.csv\n",
    "                        try:\n",
    "                            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "                            norm_latest_file = str(os.path.normpath(latest_file))\n",
    "                            print(norm_latest_file)\n",
    "                            if norm_latest_file.split('.')[-1] == 'zip':\n",
    "                                    break\n",
    "                        except FileNotFoundError:\n",
    "                            time.sleep(1)\n",
    "                    if (norm_latest_file.split('.')[-1] == 'tiff'):\n",
    "                        with open(f'{os.getcwd()}\\\\SN_{x}_{table}_{job}_RJ{reapir_job}_{page}p_{i}.pdf',\"wb\") as f:\n",
    "                            f.write(img2pdf.convert(norm_latest_file))\n",
    "                    \n",
    "                    # jesli plik jest innego rozszerzenia niz tiff, to po prostu go przenosi to odpowiedniego folderu\n",
    "                    else:\n",
    "                        src = norm_latest_file\n",
    "                        file = norm_latest_file.split('\\\\')[-1]\n",
    "                        dst = f'{os.getcwd()}\\\\{file}'\n",
    "                        shutil.move(src,dst)\n",
    "                        \n",
    "                    i += 1  \n",
    "            elif int(table) in [14, 15]:                    # było \"else:\" ale chcemy, zeby bral tylko 14, 15 i 17\n",
    "                if driver.current_url != current_url:\n",
    "                    try:\n",
    "                        download_file(driver.current_url, f'SN_{x}_{table}_{job}_{page}p_{i}')\n",
    "                    except:\n",
    "                        print('Download went wrong', x, table, job)\n",
    "                    i += 1   \n",
    "                else:\n",
    "                    \n",
    "                    downloaded = False\n",
    "                    while not downloaded:\n",
    "                        try:\n",
    "                            list_of_files = glob.glob(f'{DOWNLOAD_PATH}/*')\n",
    "                            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "                            norm_latest_file = str(os.path.normpath(latest_file))\n",
    "                            downloaded = True\n",
    "                        except FileNotFoundError:\n",
    "                            time.sleep(2)\n",
    "                    while (norm_latest_file.split('.')[-1] != 'tiff'):\n",
    "                        time.sleep(2)\n",
    "                        list_of_files = glob.glob(f'{DOWNLOAD_PATH}/*') # * means all if need specific format then *.csv\n",
    "                        try:\n",
    "                            latest_file = max(list_of_files, key=os.path.getctime)\n",
    "                            norm_latest_file = str(os.path.normpath(latest_file))\n",
    "                            print(norm_latest_file)\n",
    "                            if norm_latest_file.split('.')[-1] == 'zip':\n",
    "                                    break\n",
    "                        except FileNotFoundError:\n",
    "                            time.sleep(1)\n",
    "                    if (norm_latest_file.split('.')[-1] == 'tiff'):\n",
    "                        with open(f'{os.getcwd()}\\\\SN_{x}_{table}_{job}_{page}p_{i}.pdf',\"wb\") as f:\n",
    "                            f.write(img2pdf.convert(norm_latest_file))\n",
    "                    \n",
    "                    # jesli plik jest innego rozszerzenia niz tiff, to po prostu go przenosi to odpowiedniego folderu\n",
    "                    else:\n",
    "                        src = norm_latest_file\n",
    "                        file = norm_latest_file.split('\\\\')[-1]\n",
    "                        dst = f'{os.getcwd()}\\\\{file}'\n",
    "                        shutil.move(src,dst)\n",
    "                        \n",
    "                    i += 1\n",
    "                        \n",
    "        driver.switch_to.window(driver.window_handles[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7df08a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old function, nto in used now\n",
    "\n",
    "def download_file(download_url, filename):\n",
    "    response = urllib.request.urlopen(download_url)    \n",
    "    file = open(filename + \".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9b2d9",
   "metadata": {},
   "source": [
    "## Preparing input - list od customers and SNs\n",
    "- Example input file can be found in  01_06.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d902be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('01_06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cbd7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df[['E_Equipment Serial Number', 'E_OEM Nameplate SN', 'P_Site Customer Name', 'E_Equipment Code', 'E_Equipment Job Number']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a32c0ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.columns = ['SN_EQ', 'SN_OEM', 'Customer', 'Frame', 'Job']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fad6ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.sort_values(by='Customer').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0781fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_list = df_2['Customer'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096bba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists\n",
      "Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebryaga\\AppData\\Local\\Temp\\ipykernel_15696\\2898220153.py:47: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/html/body/form/b[2]/table[1]/tbody/tr[3]/td[1]/a\n",
      "Cannot Download ['/html/body/form/b[2]/table[1]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[1]/tbody/tr[4]/td[1]/a', '/html/body/form/b[2]/table[1]/tbody/tr[5]/td[1]/a', '/html/body/form/b[2]/table[2]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[3]/tbody/tr[3]/td[1]/a']\n",
      "Whoops\n",
      "Whoops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebryaga\\AppData\\Local\\Temp\\ipykernel_15696\\2898220153.py:47: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/html/body/form/b[2]/table[1]/tbody/tr[3]/td[1]/a\n",
      "/html/body/form/b[2]/table[2]/tbody/tr[3]/td[1]/a\n",
      "/html/body/form/b[2]/table[3]/tbody/tr[3]/td[1]/a\n",
      "C:\\Users\\ebryaga\\Downloads\\batchacc (5).tiff\n",
      "/html/body/form/b[2]/table[3]/tbody/tr[4]/td[1]/a\n",
      "C:\\Users\\ebryaga\\Downloads\\batchacc (6).tiff\n",
      "/html/body/form/b[2]/table[4]/tbody/tr[3]/td[1]/a\n",
      "C:\\Users\\ebryaga\\Downloads\\batchacc (7).tiff\n",
      "/html/body/form/b[2]/table[4]/tbody/tr[4]/td[1]/a\n",
      "/html/body/form/b[2]/table[5]/tbody/tr[3]/td[1]/a\n",
      "Cannot Download ['/html/body/form/b[2]/table[1]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[2]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[3]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[3]/tbody/tr[4]/td[1]/a', '/html/body/form/b[2]/table[4]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[4]/tbody/tr[4]/td[1]/a', '/html/body/form/b[2]/table[5]/tbody/tr[3]/td[1]/a', '/html/body/form/b[2]/table[5]/tbody/tr[4]/td[1]/a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ebryaga\\AppData\\Local\\Temp\\ipykernel_15696\\2898220153.py:47: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/html/body/form/b[2]/table[1]/tbody/tr[3]/td[1]/a\n",
      "/html/body/form/b[2]/table[1]/tbody/tr[4]/td[1]/a\n",
      "/html/body/form/b[2]/table[2]/tbody/tr[3]/td[1]/a\n",
      "C:\\Users\\ebryaga\\Downloads\\batchacc (9).tiff\n",
      "/html/body/form/b[2]/table[2]/tbody/tr[4]/td[1]/a\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n",
      "C:\\Users\\ebryaga\\Downloads\\1c345684-dddf-4a29-82f9-94918a651673.tmp\n"
     ]
    }
   ],
   "source": [
    "# Full download\n",
    "\n",
    "for customer in customer_list[:5]:\n",
    "    customer_folder = customer.replace('/','').replace('\\\\','')\n",
    "    new_folder = f'C:\\\\Users\\\\ebryaga\\\\Desktop\\\\Coll_Docu_Test\\\\{customer_folder}'\n",
    "    try:\n",
    "        os.mkdir(new_folder)\n",
    "    except:\n",
    "        print('Folder exists')\n",
    "    os.chdir(new_folder)\n",
    "    df_temp = df_2[df_2['Customer'] == customer]\n",
    "    sn_list = df_temp['SN_OEM'].to_list()\n",
    "    for x in sn_list:\n",
    "        new_folder = f'C:\\\\Users\\\\ebryaga\\\\Desktop\\\\Coll_Docu_Test\\\\{customer_folder}\\\\SN_{x}'\n",
    "        try:\n",
    "            os.mkdir(new_folder)\n",
    "        except:\n",
    "            print('Folder exists')\n",
    "        os.chdir(new_folder)\n",
    "        search_documents(x)\n",
    "        get_tables(x)\n",
    "        att_links = get_att_links(x)\n",
    "        if att_links != None:\n",
    "            try:\n",
    "                download_filter_attachements(x, att_links)\n",
    "            except:\n",
    "                print(f'Cannot Download {att_links}')\n",
    "\n",
    "        # Back to main folder\n",
    "        os.chdir(f'C:\\\\Users\\\\ebryaga\\\\Desktop\\\\Coll_Docu_Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f526c240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17573da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
